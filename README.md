# My Synopsis on Designing Machine Learning System An Iterative Process for Production-Ready Applications by Chip Huyen 

Here are my highlighted notes from the book. 

1. [CHAPTER 1: Overview of Machine Learning Systems](#1) 
2. [CHAPTER 2: Introduction to Machine Learning Systems Design](#2)
3. [CHAPTER 3: Data Engineering Fundamentals](#3)
4. [CHAPTER 4: Training Data](#4)
5. [CHAPTER 5: Feature Engineering](#5)
6. [CHAPTER 6: Model Development and Offline Evaluation](#6)
7. [CHAPTER 7: Model Deployment and Prediction Service](#7)
8. [CHAPTER 8: Data Distribution Shifts and Monitoring](#8)
9. [CHAPTER 9: Continual Learning and Test in Production](#9)
10. [CHAPTER 10: Infrastructure and Tooling for MLOps](#10)
11. [CHAPTER 11: The Human Side of Machine Learning](#11)


<a name="1"></a>
## CHAPTER 1: Overview of Machine Learning Systems

1. Many people, when they hear “machine learning system,” think of just the ML algorithms being used such as logistic regression or different types of neural networks. However, the algorithm is only a small part of an ML system in production. The system also includes the business requirements that gave birth to the ML project in the first place, the interface where users and developers interact with your system, the data stack, and the logic for developing, monitoring, and updating your models, as well as the infrastructure that enables the delivery of that logic.
2. The Relationship Between MLOps and ML Systems Design: Ops in MLOps comes from DevOps, short for Developments and Operations. To operationalize something means to bring it into production, which includes deploying, monitoring, and maintaining it. MLOps is a set of tools and best practices for bringing ML into production.
3. Before discussing how to develop an ML system, it’s important to ask a fundamental question of when and when not to use ML.
4. As its adoption in the industry quickly grows, ML has proven to be a powerful tool for a wide range of problems. Despite an incredible amount of excitement and hype generated by people both inside and outside the field, ML is not a magic tool that can solve all problems. Even for problems that ML can solve, ML solutions might not be the optimal solutions. Before starting an ML project, you might want to ask whether ML is necessary or cost-effective. To understand what ML can do, let’s examine what ML solutions generally do: Machine learning is an approach to (1) learn (2) complex patterns from (3) existing data and use these patterns to make (4) predictions on (5) unseen data.

   4.1 **Learn**: the system has the capacity to learn: A relational database isn’t an ML system because it doesn’t have the capacity to learn. You can explicitly state the relationship between two columns in a relational database, but it’s unlikely to have the capacity to figure out the relationship between these two columns by itself.

   4.2 **Complex patterns:** there are patterns to learn, and they are complex: ML solutions are only useful when there are patterns to learn. Sane people don’t invest money into building an ML system to predict the next outcome of a fair die because there’s no pattern in how these outcomes are generated. However, there are patterns in how stocks are priced, and therefore companies have invested billions of dollars in building ML systems to learn those patterns. Whether a pattern exists might not be obvious, or if patterns exist, your dataset or ML algorithms might not be sufficient to capture them.

   Consider a website like Airbnb with a lot of house listings; each listing comes with a zip code. If you want to sort listings into the states they are located in, you wouldn’t need an ML system. Since the pattern is simple—each zip code corresponds to a known state—you can just use a lookup table.

   The relationship between a rental price and all its characteristics follows a much more complex pattern, which would be very challenging to manually specify. ML is a good solution for this. Instead of telling your system how to calculate the price from a list of characteristics, you can provide prices and characteristics, and let your ML system figure out the pattern. The difference between ML solutions and the lookup table solution as well as general traditional software solutions is shown in Figure below. For this reason, ML is also called Software 2.0.

    ML has been very successful with tasks with complex patterns such as object detection and speech recognition. What is complex to machines is different from what is complex to humans. Many tasks that are hard for humans to do are easy for machines—for example, raising a number of the power of 10. On the other hand, many tasks that are easy for humans can be hard for machines—for example, deciding whether there’s a cat in a picture.

![](https://github.com/DanialArab/images/blob/main/Designing_ML_Systems/fig_1_2.png)

   4.3  **Existing data: data is available, or it’s possible to collect data**: Because ML learns from data, there must be data for it to learn from. It’s amusing to think about building a model to predict how much tax a person should pay a year, but it’s not possible unless you have access to tax and income data of a large population. In the zero-shot learning (sometimes known as zero-data learning) context, it’s
possible for an ML system to make good predictions for a task without having been trained on data for that task. However, this ML system was previously trained on data for other tasks, often related to the task in consideration. So even though the system doesn’t require data for the task at hand to learn from, it still requires data to learn.

   It’s also possible to launch an ML system without data. For example, in the context of continual learning, ML models can be deployed without having been trained on any data, but they will learn from incoming data in production.
However, serving insufficiently trained models to users comes with certain risks, such as poor customer experience.

   4.4 **Predictions: it’s a predictive problem**: ML models make predictions, so they can only solve problems that require predictive answers. ML can be especially appealing when you can benefit from a large quantity of cheap but approximate predictions. In English, “predict” means “estimate a value in the future.” For example, what will the weather be like tomorrow? Who will win the Super Bowl this year? What movie will a user want to watch next?

   As predictive machines (e.g., ML models) are becoming more effective, more and more problems are being reframed as predictive problems.

   4.5 **Unseen data: unseen data shares patterns with the training data**: The patterns your model learns from existing data are only useful if unseen data also share these patterns. A model to predict whether an app will get downloaded on Christmas 2020 won’t perform very well if it’s trained on data from 2008, when the most popular app on the App Store was Koi Pond. What’s Koi Pond? Exactly. 
   
   In technical terms, it means your unseen data and training data should come from similar distributions.

   Due to the way most ML algorithms today learn, ML solutions will especially shine if your problem has these **additional following characteristics:**

   4.6 **It’s repetitive:** Humans are great at few-shot learning: you can show kids a few pictures of cats and most of them will recognize a cat the next time they see one. Despite exciting progress in few-shot learning research, most ML algorithms still require many examples to learn a pattern. When a task is repetitive, each pattern is repeated multiple times, which makes it easier for machines to learn it.

   4.7 **The cost of wrong predictions is cheap:** Unless your ML model’s performance is 100% all the time, which is highly unlikely for any meaningful tasks, your model is going to make mistakes. ML is especially suitable when the cost of a wrong prediction is low. For example, one of the biggest use cases of ML today is in recommender systems because with recommender systems, a bad recommendation is usually forgiving—the user just won’t click on the recommendation.

   If one prediction mistake can have catastrophic consequences, ML might still be a suitable solution if, on average, the benefits of correct predictions outweigh the cost of wrong predictions. Developing self-driving cars is challenging because an algorithmic mistake can lead to death. However, many companies still want to develop self-driving cars because they have the potential to save many lives once self-driving cars are statistically safer than human drivers.


   4.8 **It’s at scale**: ML solutions often require nontrivial up-front investment on data, compute, infrastructure, and talent, so it’d make sense if we can use these solutions a lot. 
   
   “At scale” means different things for different tasks, but, in general, it means making a lot of predictions. Examples include sorting through millions of emails a year or predicting which departments thousands of support tickets should be routed to a day. 
   
   A problem might appear to be a singular prediction, but it’s actually a series of predictions. For example, a model that predicts who will win a US presidential election seems like it only makes one prediction every four years, but it might actually be making a prediction every hour or even more frequently because that prediction has to be continually updated to incorporate new information.

   4.9 **The patterns are constantly changing**: Cultures change. Tastes change. Technologies change. What’s trendy today might be old news tomorrow. Consider the task of email spam classification. Today an indication of a spam email is a Nigerian prince, but tomorrow it might be a distraught Vietnamese writer. 
   
   If your problem involves one or more constantly changing patterns, hardcoded solutions such as handwritten rules can become outdated quickly. Figuring how your problem has changed so that you can update your handwritten rules accordingly can be too expensive or impossible. Because ML learns from data, you can update your ML model with new data without having to figure out how the data has changed. It’s also possible to set up your system to adapt to the
changing data distributions, an approach we’ll discuss in the section “Continual Learning” on page 264.

   The list of use cases can go on and on, and it’ll grow even longer as ML adoption matures in the industry. Even though ML can solve a subset of problems very well, it can’t solve and/or shouldn’t be used for a lot of problems. Most of today’s ML algorithms shouldn’t be used under any of the following conditions:
   
• It’s unethical. We’ll go over one case study where the use of ML algorithms can be argued as unethical in the section “Case study I: Automated grader’s biases” on page 341.

• Simpler solutions do the trick. In Chapter 6, we’ll cover the four phases of ML model development where the first phase should be non-ML solutions.

• It’s not cost-effective.

   However, even if ML can’t solve your problem, it might be possible to break your problem into smaller components, and use ML to solve some of them. For example, if you can’t build a chatbot to answer all your customers’ queries, it might be possible to build an ML model to predict whether a query matches one of the frequently asked questions. If yes, direct the customer to the answer. If not, direct them to customer
service.

5. Machine Learning Use Cases: ML has found increasing usage in both enterprise and consumer applications
6. Latency vs accuracy trade-off: Even though the market for consumer ML applications is booming, the majority of ML use cases are still in the enterprise world. Enterprise ML applications tend to have vastly different requirements and considerations from consumer applications. There are many exceptions, but for most cases, enterprise applications might have stricter accuracy requirements but be more forgiving with latency requirements. For example, improving a speech recognition system’s accuracy from 95% to 95.5% might not be noticeable to most consumers, but improving a resource allocation system’s efficiency by just 0.1% can help a corporation like Google or General Motors save millions of dollars. At the same time, latency of a second might get a consumer distracted and opening something else, but enterprise users might be more tolerant of high latency.
7. According to Algorithmia’s 2020 state of enterprise machine learning survey, ML applications in enterprises are diverse, serving both internal use cases (reducing costs, generating customer insights and intelligence, internal processing automation) and external use cases (improving customer experience, retaining customers, interacting with customers).
8. Deciding how much to charge for your product or service is probably one of the hardest business decisions; why not let ML do it for you? Price optimization is the process of estimating a price at a certain time period to maximize a defined objective function, such as the company’s margin, revenue, or growth rate. ML-based pricing optimization is most suitable for cases with a large number of transactions where demand fluctuates and consumers are willing to pay a **dynamic price**—for example, internet ads, flight tickets, accommodation bookings, ride-sharing, and events.
9. Reducing customer acquisition costs by a small amount can result in a large increase in profit. This can be done through better identifying potential customers, showing better-targeted ads, giving discounts at the right time, etc.—all of which are suitable tasks for ML. After you’ve spent so much money acquiring a customer, it’d be a shame if they leave. The cost of acquiring a new user is approximated to be 5 to 25 times more expensive than retaining an existing one. Churn prediction is predicting when a specific customer is about to stop using your products or services so that you can take appropriate actions to win them back. Churn prediction can be used not only for customers but also for employees.
10. **Understanding Machine Learning Systems**: Understanding ML systems will be helpful in designing and developing them. In this section, we’ll go over how ML systems are different from both ML in research (or as often taught in school) and traditional software, which motivates the need for this book.
11. **Machine Learning in Research Versus in Production**: ML in production is very different from ML in research. The table below shows five of the major differences.

![](https://github.com/DanialArab/images/blob/main/Designing_ML_Systems/table_1_1.png)

12. **Different stakeholders and requirements**: People involved in a research and leaderboard project often align on one single objective. The most common objective is model performance—develop a model that
achieves the state-of-the-art results on benchmark datasets. To edge out a small improvement in performance, researchers often resort to techniques that make models too complex to be useful.

There are many stakeholders involved in bringing an ML system into production. Each stakeholder has their own requirements. Having **different, often conflicting, requirements can make it difficult to design, develop, and select an ML model that satisfies all the requirements.** Consider a mobile app that recommends restaurants to users. The app makes money by charging restaurants a 10% service fee on each order. This means that expensive orders give the app more money than cheap orders. The project involves ML engineers, salespeople, product managers, infrastructure engineers, and a manager:

**ML engineers**: Want a model that recommends restaurants that users will most likely order from, and they believe they can do so by using a more complex model with more data.

**Sales team**: Wants a model that recommends the more expensive restaurants since these restaurants bring in more service fees.

**Product team**: Notices that every increase in latency leads to a drop in orders through the service, so they want a model that can return the recommended restaurants in less than 100 milliseconds.

**ML platform team**: As the traffic grows, this team has been woken up in the middle of the night because of problems with scaling their existing system, so they want to hold off on model updates to prioritize improving the ML platform.

**Manager**: Wants to maximize the margin, and one way to achieve this might be to let go of the ML team.

“Recommending the restaurants that users are most likely to click on” and “recommending the restaurants that will bring in the most money for the app” are two different objectives, and in the section **“Decoupling objectives”** on page 41, we’ll discuss how to develop an ML system that satisfies different objectives. **Spoiler: we’ll develop one model for each objective and combine their predictions.**

13. When developing an ML project, it’s important for ML engineers to understand requirements from all stakeholders involved and how strict these requirements are. For example, if being able to return recommendations within 100 milliseconds is a must-have requirement—the company finds that if your model takes over 100 milliseconds to recommend restaurants, 10% of users would lose patience and close the app—then neither model A nor model B will work. However, if it’s just a nice-to-have requirement, you might still want to consider model A or model B.

Production having different requirements from research is one of the reasons why successful research projects might not always be used in production. For example, ensembling is a technique popular among the winners of many ML competitions, including the famed $1 million Netflix Prize, and yet it’s not widely used in production. Ensembling combines “multiple learning algorithms to obtain better predictive performance than could be obtained from any of the constituent learning algorithms alone.” While it can give your ML system a small performance improvement, ensembling tends to make a system too complex to be useful in production, e.g., slower to make predictions or harder to interpret the results. We’ll discuss ensembling further in the section “Ensembles” on page 156.

14. **Computational priorities**: When designing an ML system, people who haven’t deployed an ML system often make the mistake of focusing too much on the model development part and not enough on the model deployment and maintenance part.

During the model development process, you might train many different models, and each model does multiple passes over the training data. Each trained model then generates predictions on the validation data once to report the scores. The validation data is usually much smaller than the training data. **During model development, training is the bottleneck**. **Once the model has been deployed, however, its job is to generate predictions, so inference is the bottleneck**. **Research usually prioritizes fast training, whereas production usually prioritizes fast inference.**

One corollary of this is that **research prioritizes high throughput whereas production prioritizes low latency**. In case you need a refresh, **latency refers to the time it takes from receiving a query to returning the result. Throughput refers to how many queries are processed within a specific period of time.**

15. **If your system always processes one query at a time, higher latency means lower throughput**. If the average latency is 10 ms, which means it takes 10 ms to process a query, the throughput is 100 queries/second. If the average latency is 100 ms, the throughput is 10 queries/second.

However, because most modern distributed systems **batch queries** to process them together, often concurrently, **higher latency might also mean higher throughput**. If you process 10 queries at a time and it takes 10 ms to run a batch, the average latency is still 10 ms but the throughput is now 10 times higher—1,000 queries/second. If you process 50 queries at a time and it takes 20 ms to run a batch, the average latency now is 20 ms and the throughput is 2,500 queries/second. Both latency and throughput have increased! The difference in latency and throughput trade-off for processing queries one at a time and processing queries in batches is illustrated in Figure 1-4.

![](https://github.com/DanialArab/images/blob/main/Designing_ML_Systems/fig_1_4.png)

**In research, you care more about how many samples you can process in a second (throughput) and less about how long it takes for each sample to be processed (latency)**. You’re willing to increase latency to increase throughput, for example, with aggressive batching.

**However, once you deploy your model into the real world, latency matters a lot.**

17. To reduce latency in production, you might have to reduce the number of queries you can process on the same hardware at a time. If your hardware is capable of processing many more queries at a time, using it to process fewer queries means underutilizing your hardware, increasing the cost of processing each query. 

When thinking about latency, it’s important to keep in mind that latency is not an individual number but a distribution. It’s tempting to simplify this distribution by using a single number like the average (arithmetic mean) latency of all the requests within a time window, but this number can be misleading. Imagine you have 10 requests whose latencies are 100 ms, 102 ms, 100 ms, 100 ms, 99 ms, 104 ms, 110 ms, 90 ms, 3,000 ms, 95 ms. The average latency is 390 ms, which makes your system seem slower than it actually is. What might have happened is that there was a network error that made one request much slower than others, and you should investigate that troublesome request.

It’s usually better to think in percentiles, as they tell you something about a certain percentage of your requests. The most common percentile is the 50th percentile, abbreviated as p50. It’s also known as the median. If the median is 100 ms, half of the requests take longer than 100 ms, and half of the requests take less than 100 ms. 

Higher percentiles also help you discover outliers, which might be symptoms of something wrong. Typically, the percentiles you’ll want to look at are p90, p95, and p99. The 90th percentile (p90) for the 10 requests above is 3,000 ms, which is an outlier.

Higher percentiles are important to look at because even though they account for a small percentage of your users, sometimes they can be the most important users. For example, on the Amazon website, the customers with the slowest requests are often those who have the most data on their accounts because they have made many purchases—that is, they’re the most valuable customers.

It’s a common practice to use high percentiles to specify the performance requirements for your system; for example, a product manager might specify that the 90th percentile or 99.9th percentile latency of a system must be below a certain number.

18. **Data**: During the research phase, the datasets you work with are often clean and wellformatted, freeing you to focus on developing models. They are static by nature so that the community can use them to benchmark new architectures and techniques. This means that many people might have used and discussed the same datasets, and quirks of the dataset are known. You might even find open source scripts to process and feed the data directly into your models.

In production, data, if available, is a lot more messy. It’s noisy, possibly unstructured, constantly shifting. It’s likely biased, and you likely don’t know how it’s biased. Labels, if there are any, might be sparse, imbalanced, or incorrect. Changing project or business requirements might require updating some or all of your existing labels. If you work with users’ data, you’ll also have to worry about privacy and regulatory concerns. 

19. **Fairness**: During the research phase, a model is not yet used on people, so it’s easy for researchers to put off fairness as an afterthought: “Let’s try to get state of the art first and worry about fairness when we get to production.” When it gets to production, it’s too late. If you optimize your models for better accuracy or lower latency, you can show that your models beat state of the art. But, as of writing this book, there’s no equivalent state of the art for fairness metrics.

20. **Interpretability:** In early 2020, the Turing Award winner Professor Geoffrey Hinton proposed a heatedly debated question about the importance of interpretability in ML systems. “Suppose you have cancer and you have to choose between a black box AI surgeon that cannot explain how it works but has a 90% cure rate and a human surgeon with an 80% cure rate. Do you want the AI surgeon to be illegal?”

A couple of weeks later, when I asked this question to a group of 30 technology executives at public nontech companies, only half of them would want the highly effective but unable-to-explain AI surgeon to operate on them. The other half wanted the human surgeon.

While most of us are comfortable with using a microwave without understanding how it works, many don’t feel the same way about AI yet, especially if that AI makes important decisions about their lives.

21. Some might argue that it’s OK to know only the academic side of ML because there are plenty of jobs in research. The first part—it’s OK to know only the academic side of ML—is true. The second part is false.

22. **Machine Learning Systems Versus Traditional Software**: Since ML is part of software engineering (SWE), and software has been successfully used in production for more than half a century, some might wonder why we don’t just take tried-and-true best practices in software engineering and apply them to ML.

That’s an excellent idea. In fact, ML production would be a much better place if ML experts were better software engineers. Many traditional SWE tools can be used to develop and deploy ML applications.

However, many challenges are unique to ML applications and require their own tools. In SWE, **there’s an underlying assumption that code and data are separated.** In fact, in SWE, **we want to keep things as modular and separate as possible** (see the Wikipedia page on separation of concerns). On the contrary, **ML systems are part code, part data, and part artifacts created from the two**. The trend in the last decade shows that applications developed with the most/best data win. Instead of focusing on improving ML algorithms, most companies will focus on improving their data. Because data can change quickly, ML applications need to be adaptive to the changing environment, which might require faster development and deployment cycles.

**In traditional SWE, you only need to focus on testing and versioning your code. With ML, we have to test and version our data too, and that’s the hard part.** How to version large datasets? How to know if a data sample is good or bad for your system? Not all data samples are equal—some are more valuable to your model than others. For example, if your model has already trained on one million scans of normal lungs and only one thousand scans of cancerous lungs, a scan of a cancerous lung is much more valuable than a scan of a normal lung. Indiscriminately accepting all available data might hurt your model’s performance and even make it susceptible to data poisoning attacks. 

The size of ML models is another challenge. As of 2022, it’s common for ML models to have hundreds of millions, if not billions, of parameters, which requires gigabytes of random-access memory (RAM) to load them into memory. A few years from now, a billion parameters might seem quaint—like, “Can you believe the computer that sent men to the moon only had 32 MB of RAM?”

However, for now, getting these large models into production, especially on edge devices, is a massive engineering challenge. Then there is the question of how to get these models to run fast enough to be useful. An autocompletion model is useless if the time it takes to suggest the next character is longer than the time it takes for you to type.

The good news is that these engineering challenges are being tackled at a breakneck pace. Back in 2018, when the Bidirectional Encoder Representations from Transformers (BERT) paper first came out, people were talking about how BERT was too big, too complex, and too slow to be practical. The pretrained large BERT model has 340 million parameters and is 1.35 GB.33 Fast-forward two years later, BERT and its variants were already used in almost every English search on Google.

<a name="2"></a>
## CHAPTER 2: Introduction to Machine Learning Systems Design

1. To reiterate from the first chapter, ML systems design takes a system approach to MLOps, which means that we’ll consider an ML system holistically to ensure that all the components—the business requirements, the data stack, infrastructure, deployment, monitoring, etc.— and their stakeholders can work together to satisfy the specified objectives and requirements.

Before we develop an ML system, we must understand **why this system is needed**. If this system is built for a business, it must be driven by **business objectives, which will need to be translated into ML objectives to guide the development of ML models.**

Once everyone is on board with the objectives for our ML system, we’ll need to set out some **requirements to guide the development of this system**. In this book, we’ll consider the four requirements: **reliability, scalability, maintainability, and adaptability.** We will then introduce the iterative process for designing systems to meet those requirements.

You might wonder: with all these objectives, requirements, and processes in place, can I finally start building my ML model yet? Not so soon! Before using ML algorithms to solve your problem, you first need to **frame your problem into a task that ML can solve**. We’ll continue this chapter with how to frame your ML problems. **The difficulty of your job can change significantly depending on how you frame your problem.**

Because ML is a data-driven approach, a book on ML systems design will be amiss if it fails to discuss the importance of data in ML systems. The last part of this chapter touches on a debate that has consumed much of the ML literature in recent years: **which is more important—data or intelligent algorithms?**

2. **Business and ML Objectives**: We first need to consider the objectives of the proposed ML projects. When working on an ML project, data scientists tend to care about the ML objectives: the metrics they can measure about the performance of their ML models such as accuracy, F1 score, inference latency, etc. They get excited about improving their model’s accuracy from 94% to 94.2% and might spend a ton of resources—data, compute, and engineering time—to achieve that. But the truth is: most companies don’t care about the fancy ML metrics. They don’t care about increasing a model’s accuracy from 94% to 94.2% unless it moves some **business metrics.** A pattern I see in many short-lived ML projects is that the data scientists become too focused on hacking ML metrics without paying attention to business metrics. Their managers, however, only care about business metrics and, after failing to see how an ML project can help push their business metrics, kill the projects prematurely (and possibly let go of the data science team involved)

The sole purpose of businesses, according to the Nobel-winning economist Milton Friedman, is to **maximize profits for shareholders**. The ultimate goal of any project within a business is, therefore, to increase profits,
either directly or indirectly: directly such as increasing sales (conversion rates) and cutting costs; indirectly such as higher customer satisfaction and increasing time spent on a website.

One of the reasons why predicting ad click-through rates and fraud detection are among the most popular use cases for ML today is that it’s **easy to map ML models’ performance to business metrics**: every increase in click-through rate results in actual ad revenue, and every fraudulent transaction stopped results in actual money saved.

**Many companies create their own metrics to map business metrics to ML metrics.** For example, Netflix measures the performance of their recommender system using take-rate: the number of quality plays divided by the number of recommendations a user sees.4 The higher the take-rate, the better the recommender system. Netflix also put a recommender system’s take-rate in the context of their other business metrics like total streaming hours and subscription cancellation rate. They found that a higher take-rate also results in higher total streaming hours and lower subscription cancellation rates.

To gain a definite answer on the question of how ML metrics influence business metrics, experiments are often needed. Many companies do that with experiments **like A/B testing and choose the model that leads to better business metrics, regardless of whether this model has better ML metrics.**

Returns on investment in ML depend a lot on the **maturity stage of adoption**. The longer you’ve adopted ML, the more efficient your pipeline will run, the faster your development cycle will be, the less engineering time you’ll need, and the lower your cloud bills will be, which all lead to higher returns. 

3. **Requirements for ML Systems**: Reliability -- scalability -- maintainability -- adaptability

Reliability: ML systems can fail **silently.**

Scalability: There are multiple ways an ML system can grow. It can grow in **complexity**. Last year you used a logistic regression model that fit into an Amazon Web Services (AWS) free tier instance with 1 GB of RAM, but this year, you switched to a 100-million-parameter neural network that requires 16 GB of RAM to generate predictions. Your ML system can grow **in traffic volume**. When you started deploying an ML system, you only served 10,000 prediction requests daily. However, as your company’s user base grows, the number of prediction requests your ML system serves daily fluctuates between 1 million and 10 million.

An ML system might grow in **ML model count**. Initially, you might have only one model for one use case, such as detecting the trending hashtags on a social network site like Twitter. However, over time, you want to add more features to this use case, so you’ll add one more to filter out NSFW (not safe for work) content and another model to filter out tweets generated by bots. This growth pattern is especially common in ML systems that target enterprise use cases. Initially, a startup might serve only one enterprise customer, which means this startup only has one model. However, as this startup gains more customers, they might have one model for each customer. A startup I worked with had 8,000 models in production for their 8,000 enterprise customers. 

Whichever way your system grows, there should be reasonable ways of dealing with that growth. When talking about scalability most people think of resource scaling, which consists of up-scaling (expanding the resources to handle growth) and downscaling (reducing the resources when not needed).

However, handling growth **isn’t just resource scaling**, but also **artifact management**. Managing one hundred models is very different from managing one model. With one model, you can, perhaps, manually monitor this model’s performance and manually update the model with new data. Since there’s only one model, you can just have a file that helps you reproduce this model whenever needed. However, with one hundred models, both the **monitoring and retraining aspect** will need to be **automated**. You’ll need a way to manage the code generation so that you can adequately reproduce a model when you need to.

Maintainability: It’s important to structure your workloads and set up your infrastructure in such a way that different contributors can work using tools that they are comfortable with, instead of one group of contributors forcing their tools onto other groups. **Code should be documented. Code, data, and artifacts should be versioned.** Models should be sufficiently reproducible so that even when the original authors are not around, other contributors can have sufficient contexts to build on their work. When a problem occurs, different contributors should be able to work together to identify the problem and implement a solution without finger-pointing.

Adaptability: To adapt to shifting data distributions and business requirements, the system shouldhave some capacity for both discovering aspects for performance improvement and allowing updates without service interruption.
**Because ML systems are part code, part data, and data can change quickly, ML systems need to be able to evolve quickly.**

4. Iterative Process: Developing an ML system is an iterative and, in most cases, never-ending process. Once a system is put into production, it’ll need to be continually monitored and updated. Before deploying my first ML system, I thought the process would be linear and straightforward. I thought all I had to do was to collect data, train a model, deploy that model, and be done. However, I soon realized that the process looks more like a cycle with a lot of back and forth between different steps.

Figure 2-2 shows an oversimplified representation of what the iterative process for developing ML systems in production looks like from the perspective of a data scientist or an ML engineer. This process looks different from the perspective of an ML platform engineer or a DevOps engineer, as they might not have as much context into model development and might spend a lot more time on setting up infrastructure.

![](https://github.com/DanialArab/images/blob/main/Designing_ML_Systems/fig_2_2.png)

5. **Framing ML Problems**

Imagine you’re an ML engineering tech lead at a bank that targets millennial users. One day, your boss hears about a rival bank that uses ML to speed up their customer service support that supposedly helps the rival bank process their customer requests two times faster. He orders your team to look into using ML to speed up your customer service support too. 

Slow customer support is a problem, but it’s not an ML problem. **An ML problem is defined by inputs, outputs, and the objective function that guides the learning process—none of these three components are obvious from your boss’s request**. It’s your job, as a seasoned ML engineer, to use your knowledge of what problems ML can solve to frame this request as an ML problem. 

Upon investigation, you discover that the bottleneck in responding to customer requests lies in routing customer requests to the right department among four departments: accounting, inventory, HR (human resources), and IT. You can alleviate this bottleneck by developing an ML model to predict which of these four departments a request should go to. **This makes it a classification problem. The input is the customer request. The output is the department the request should go to. The objective function is to minimize the difference between the predicted department and the actual department.**

6. **Types of ML Tasks**: HERE


<a name="3"></a>
## CHAPTER 3: Data Engineering Fundamentals

<a name="4"></a>
## CHAPTER 4: Training Data

<a name="5"></a>
## CHAPTER 5: Feature Engineering

<a name="6"></a>
## CHAPTER 6: Model Development and Offline Evaluation

<a name="7"></a>
## CHAPTER 7:Model Deployment and Prediction Service



<a name="8"></a>
## CHAPTER 8: Data Distribution Shifts and Monitoring

1. **An important lesson**: Deploying a model isn’t the end of the process. A model’s performance degrades over time in production. Once a model has been deployed, we still have to continually monitor its performance to detect issues as well as deploy updates to fix these issues.

2. **Causes of ML System Failures**: Before we identify the cause of ML system failures, let’s briefly discuss what an ML system failure is. A **failure happens when one or more expectations of the system is violated**. In traditional software, we mostly care about a **system’s operational expectations**: whether the system executes its logic within the expected operational metrics, e.g., **latency and throughput.** For an ML system, we care about **both its operational metrics and its ML performance metrics**. For example, consider an English-French machine translation system. Its operational expectation might be that, given an English sentence, the system returns a French translation within a one-second latency. Its ML performance expectation is that the returned translation is an accurate translation of the original English sentence 99% of the time.

Operational expectation violations are easier to detect, as they’re usually accompanied by an operational breakage such as a 
- timeout,
- a 404 error on a webpage,
- an out-of-memory error,
- or a segmentation fault.

However, ML performance expectation violations are harder to detect as doing so requires measuring and monitoring the performance of ML models in production. In the preceding example of the English- French machine translation system, detecting whether the returned translations are correct 99% of the time is difficult if we don’t know what the correct translations are supposed to be. There are countless examples of Google Translate’s painfully
wrong translations being used by users because they aren’t aware that these are wrong translations. For this reason, we say that **ML systems often fail silently.**

To effectively detect and fix ML system failures in production, it’s useful to understand why a model, after proving to work well during development, would fail in production. We’ll examine two types of failures: software system failures and ML-specific failures.

3. **Software system failures**:
   
- Dependency failure,
- Deployment failure (Failures caused by deployment errors, such as when you accidentally deploy the binaries of an older version of your model instead of the current version, or when your systems don’t have the right permissions to read or write certain files.)
- Hardware failure, and
- Downtime or crashing 

4. Just because some failures are not specific to ML doesn’t mean they’re not important for ML engineers to understand. In 2020, Daniel Papasian and Todd Underwood, two ML engineers at Google, looked at 96 cases where a large ML pipeline at Google broke. They reviewed data from over the previous 15 years to determine the causes and found out that 60 out of these 96 failures happened due to causes not directly related to ML.4 Most of the issues are related to distributed systems, e.g.,
- where the workflow scheduler or orchestrator makes a mistake,
- or related to the data pipeline, e.g., where data from multiple sources is joined incorrectly or the wrong data structures are being used.

Addressing software system failures requires not ML skills, but traditional software engineering skills, and addressing them is beyond the scope of this book. Because of the importance of traditional software engineering skills in deploying ML systems, ML engineering is mostly engineering, not ML.

A reason for the prevalence of software system failures is that because ML adoption in the industry is still nascent, tooling around ML production is limited and best practices are not yet well developed or standardized. However, as toolings and best practices for ML production mature, there are reasons to believe that the proportion of software system failures will decrease and the proportion of ML-specific failures will increase.

5. **ML-Specific Failures**: 

ML-specific failures are failures specific to ML systems. Examples include data collection and processing problems, poor hyperparameters, changes in the training pipeline not correctly replicated in the inference pipeline and vice versa, data distribution shifts that cause a model’s performance to deteriorate over time, edge cases, and degenerate feedback loops.

In this chapter, we’ll focus on addressing ML-specific failures. Even though they account for a small portion of failures, they can be more dangerous than non-ML failures as they’re hard to detect and fix, and they can prevent ML systems from being used altogether. We’ve covered data problems in great detail in Chapter 4, hyperparameter tuning in Chapter 6, and the danger of having two separate pipelines for training and inference in Chapter 7. In this chapter, we’ll discuss three new but very common problems that arise after a model has been deployed: 

- production data differing from training data,
- edge cases, and
- degenerate feedback loops.

6. **Production data differing from training data**

When we say that an ML model learns from the training data, it means that the model **learns the underlying distribution of the training data with the goal of leveraging this learned distribution to generate accurate predictions** for unseen data—data that it didn’t see during training.

One of the first things I learned in ML courses is that it’s **essential for the training data and the unseen data to come from a similar distribution**. The assumption is that the unseen data comes from a **stationary distribution that is the same as the training data distribution**. If the unseen data comes from a different distribution, the model might not generalize well.

This assumption is incorrect in most cases for two reasons:

- First, the underlying distribution of the real-world data is unlikely to be the same as the underlying distribution of the training data. Curating a training dataset that can accurately represent the data that a model will encounter in production turns out to be very difficult. Real-world data is multifaceted and, in many cases, virtually infinite, **whereas training data is finite and constrained by the time, compute, and human resources available during the dataset creation and processing**. There are many different selection and sampling biases, as discussed in Chapter 4, that can happen and make real-world data diverge from training data. The divergence can be something as minor as real-world data using a different type of encoding of emojis. This type of divergence leads to a common failure mode known as the **train-serving skew: a model that does great in development but performs poorly when deployed.**

- Second, the real world **isn’t stationary**. Things change. Data distributions shift. In 2019, when people searched for Wuhan, they likely wanted to get travel information, but since COVID-19, when people search for Wuhan, they likely want to know about the place where COVID-19 originated. Another common failure mode is that a model does great when first deployed, but its performance degrades over time as the data distribution changes. This failure mode needs to be **continually monitored and detected for as long as a model remains in production.**

When I use COVID-19 as an example that causes data shifts, some people have the impression that data shifts only happen because of **unusual events, which implies they don’t happen often. Data shifts happen all the time, suddenly, gradually, or seasonally**. They can happen suddenly because of a specific event, such as when your existing competitors change their pricing policies and you have to update your price predictions in response, or when you launch your product in a new region, or when a celebrity mentions your product, which causes a surge in new users, and so on. They can happen gradually because social norms, cultures, languages, trends, industries, etc. just change over time. They can also happen due to seasonal variations, such as people might be more likely to request rideshares in the winter when it’s cold and snowy than in the spring.

Due to the complexity of ML systems and the poor practices in deploying them, a large percentage of what might look like data shifts on monitoring dashboards are caused by **internal errors, such as bugs in the data pipeline, missing values incorrectly inputted, inconsistencies between the features extracted during training and inference, features standardized using statistics from the wrong subset of data, wrong model version, or bugs in the app interface that force users to change their behaviors.**

7. **Edge cases**
   
Imagine there existed a self-driving car that can drive you safely 99.99% of the time, but the other 0.01% of the time, it might get into a catastrophic accident that can leave you permanently injured or even dead. Would you use that car?

If you’re tempted to say no, you’re not alone. An ML model that performs well on most cases but fails on a small number of cases might not be usable if these failures cause catastrophic consequences. For this reason, **major self-driving car companies are focusing on making their systems work on edge cases.**

**Edge cases are the data samples so extreme that they cause the model to make catastrophic mistakes.** Even though edge cases generally refer to data samples drawn from the same distribution, if there is a sudden increase in the number of data samples in which your model doesn’t perform well, it could be an indication that the underlying data distribution has shifted.

Autonomous vehicles are often used to illustrate how edge cases can prevent an ML system from being deployed. But this is also true for any safety-critical application such as medical diagnosis, traffic control, e-discovery, etc. It can also be true for non-safety-critical applications. Imagine a customer service chatbot that gives reasonable responses to most of the requests, but sometimes, it spits out outrageously racist or sexist content. This chatbot will be a brand risk for any company that wants to use it, thus rendering it unusable.

8. **Edge Cases and Outliers**
   
You might wonder about the differences between an outlier and an edge case. The definition of what makes an edge case varies by discipline. In ML, because of its recent adoption in production, edge cases are still being discovered, which makes their definition contentious. 

In this book, **outliers refer to data**: an example that differs significantly from other examples. **Edge cases refer to performance**: an example where a **model performs significantly worse than other examples**. **An outlier can cause a model to perform unusually poorly, which makes it an edge case. However, not all outliers are edge cases.** For example, **a person jaywalking on a highway is an outlier, but it’s not an edge case if your self-driving car can accurately detect that person and decide on a motion response appropriately.**

During model development, outliers can negatively affect your model’s performance, as shown in Figure 8-1. In many cases, it might be beneficial to remove outliers as it helps your model to learn better decision boundaries and generalize better to unseen data. However, during inference, you don’t usually have the option to remove or ignore the queries that differ significantly from other queries. You can choose to transform it—for example, when you enter “mechin learnin” into Google Search, Google might ask if you mean “machine learning.” But most likely you’ll want to develop a model so that it can perform well even on unexpected inputs.

![](https://github.com/DanialArab/images/blob/main/Designing_ML_Systems/figure_8_1.png)

9. **Degenerate feedback loops**:

A degenerate feedback loop can happen when the **predictions themselves influence the feedback, which, in turn, influences the next iteration of the model**. More formally, a degenerate feedback loop is created when a system’s outputs are used to generate the system’s future inputs, which, in turn, influence the system’s future outputs. In ML, a system’s predictions can influence how users interact with the system, and because users’ interactions with the system are sometimes used as training data to the same system, degenerate feedback loops can occur and cause unintended consequences.

To make this concrete, imagine you build a system to recommend to users songs that they might like. The songs that are ranked high by the system are shown first to users. Because they are shown first, users click on them more, which makes the system more confident that these recommendations are good. In the beginning, the rankings of two songs, A and B, might be only marginally different, but because A was originally ranked a bit higher, it showed up higher in the recommendation list, making users click on A more, which made the system rank A even higher. After a while, A’s ranking became much higher than B’s.13 Degenerate feedback loops are one reason why popular movies, books, or songs keep getting more popular, which makes it hard for new items to break into popular lists. This type of scenario is incredibly common in production, and it’s heavily researched. It goes by many different names, including “exposure bias,” “popularity bias,” “filter bubbles,” and sometimes “echo chambers.”

10. **Detecting degenerate feedback loops.** If degenerate feedback loops are so bad, how do we know if a feedback loop in a system is degenerate? When a system is offline, degenerate feedback loops are difficult to detect. Degenerate loops result from user feedback, and a system won’t have users until it’s online (i.e., deployed to users).

For the task of recommender systems, it’s possible to detect degenerate feedback loops by measuring the popularity diversity of a system’s outputs even when the system is offline. An item’s popularity can be measured based on how many times it has been interacted with (e.g., seen, liked, bought, etc.) in the past. The popularity of all the items will likely follow a long-tail distribution: a small number of items are interacted with a lot, while most items are rarely interacted with at all. Various metrics such as aggregate diversity and average coverage of long-tail items proposed by Brynjolfsson et al. (2011), Fleder and Hosanagar (2009), and Abdollahpouri et al. (2019) can help you measure the diversity of the outputs of a recommender system.15 Low scores mean that the outputs of your system are homogeneous, which might be caused by popularity bias.

11. **Correcting degenerate feedback loops.** Because degenerate feedback loops are a common problem, there are many proposed methods on how to correct them. In this chapter, we’ll discuss two methods. The first one is to use **randomization, and the second one is to use positional features.**

12. **Randomization** 

We’ve discussed that **degenerate feedback loops can cause a system’s outputs to be more homogeneous over time.** Introducing randomization in the predictions can reduce their homogeneity. In the case of recommender systems, instead of showing the users only the items that the system ranks highly for them, we show users random items and use their feedback to determine the true quality of these items. This is the approach that TikTok follows. Each new video is randomly assigned an initial pool of traffic (which can be up to hundreds of impressions). This pool of traffic is used to evaluate each video’s unbiased quality to determine whether it should be moved to a bigger pool of traffic or be marked as irrelevant.

**Randomization has been shown to improve diversity, but at the cost of user experience.** Showing our users completely random items might cause users to lose interest in our product. An intelligent exploration strategy, such as those discussed in the section “Contextual bandits as an exploration strategy” on page 289, can help increase item diversity with acceptable prediction accuracy loss. Schnabel et al. use a small amount of randomization and causal inference techniques to estimate the unbiased value of each song.19 They were able to show that this algorithm was able to correct a recommender system to make recommendations fair to creators.

13. **Positional features**

We’ve also discussed that degenerate feedback loops are caused by users’ feedback on predictions, and users’ feedback on a prediction is biased based on **where it is shown.** Consider the preceding recommender system example, where each time you recommend five songs to users. You realize that the top recommended song is much more likely to be clicked on compared to the other four songs. You are unsure whether your model is exceptionally good at picking the top song, or whether users click on any song as long as it’s recommended on top.


**If the position in which a prediction is shown affects its feedback in any way, you might want to encode the position information using positional features**. Positional features can be **numerical (e.g., positions are 1, 2, 3,...) or Boolean (e.g., whether a prediction is shown in the first position or not).**

14. **Data distribution shifts**

In the previous section, we discussed common causes for ML system failures. In this section, we’ll zero in onto one especially sticky cause of failures: **data distribution shifts, or data shifts for short**. Data distribution shift refers to the phenomenon in supervised learning when the data a model works with changes over time, which causes this model’s predictions to become less accurate as time passes. The distribution of the data the model is trained on is called the **source distribution**. The distribution of the data the model runs inference on is called the **target distribution.**

15. **Types of Data Distribution Shifts**:

While data distribution shift is often used interchangeably with 
- concept drift and
- covariate shift and occasionally
- label shift,

these are three distinct subtypes of data shift. Note that this discussion on different types of data shifts is math-heavy and mostly useful from a research perspective: to develop efficient algorithms to detect and address data shifts requires understanding the causes of those shifts. **In production, when encountering a distribution shift, data scientists don’t usually stop to wonder what type of shift it is. They mostly care about what they can do to handle this shift.**

16. **Covariate shift**
    
When P(X) changes but P(Y|X) remains the same. This refers to the first decomposition of the joint distribution.

In statistics, **a covariate is an independent variable that can influence the outcome of a given statistical trial but which is not of direct interest**. Consider that you are running an experiment to determine how locations affect the housing prices. The housing price variable is your direct interest, but you know the square footage affects the price, so the square footage is a covariate. In supervised ML, the label is the variable of direct interest, and the input features are covariate variables.

Mathematically, covariate shift is when P(X) changes, but P(Y|X) remains the same, which means that the distribution of the input changes, but the conditional probability of an output given an input remains the same.

During model development, covariate shifts can happen due to biases during the data selection process, which could result from difficulty in collecting examples for certain classes. For example, suppose that to study breast cancer, you get data from a clinic where women go to test for breast cancer. Because people over 40 are encouraged by their doctors to get checkups, your data is dominated by women over 40. For this reason, covariate shift is closely related to the sample selection bias problem.

Covariate shifts can also happen because the training data is artificially altered to make it easier for your model to learn. As discussed in Chapter 4, it’s hard for ML models to learn from **imbalanced datasets**, so you might want to collect more samples of the rare classes or oversample your data on the rare classes to make it easier for your model to learn the rare classes.

Covariate shift can also be caused by the model’s learning process, especially through active learning. In Chapter 4, we defined active learning as follows: instead of randomly selecting samples to train a model on, we use the samples most helpful to that model according to some heuristics. This means that the training input distribution is altered by the learning process to differ from the real-world input distribution, and covariate shifts are a by-product. 

In production, covariate shift usually happens because of major changes in the environment or in the way your application is used. Imagine you have a model to predict how likely a free user will be to convert to a paid user. The income level of the user is a feature. Your company’s marketing department recently launched a campaign that attracts users from a demographic more affluent than your current demographic. The input distribution into your model has changed, but the probability that a user with a given income level will convert remains the same.

If you know in advance how the real-world input distribution will differ from your training input distribution, you can leverage techniques such as **importance weighting** to train your model to work for the real-world data. Importance weighting consists of two steps: estimate the density ratio between the real-world input distribution and the training input distribution, then weight the training data according to this ratio and train an ML model on this weighted data.

However, because we don’t know in advance how the distribution will change in the real world, it’s very difficult to preemptively train your models to make them robust to new, unknown distributions. There has been research that attempts to help models learn representations of latent variables that are invariant across data distributions, but I’m not aware of their adoption in the industry.

17. **Label shift, also known as prior shift, prior probability shift, or target shift**
    
When P(Y) changes but P(X|Y) remains the same. This refers to the second decomposition of the joint distribution.

You can think of this as the case when the output distribution changes but, for a given output, the input distribution stays the same.

Remember that covariate shift is when the input distribution changes. When the input distribution changes, the output distribution also changes, resulting in both covariate shift and label shift happening at the same time.

However, not all covariate shifts result in label shifts. It’s a subtle point, so we’ll consider another example.

Because label shift is closely related to covariate shift, methods for detecting and adapting models to label shifts are similar to covariate shift adaptation methods. We’ll discuss them more later in this chapter.

18. **Concept drift also known as posterior shift**
    
When P(Y|X) changes but P(X) remains the same. This refers to the first decomposition of the joint distribution.

Concept drift, also known as posterior shift, is when the input distribution remains the same but the conditional distribution of the output given an input changes. You can think of this as **“same input, different output.”** Consider you’re in charge of a model that predicts the price of a house based on its features. Before COVID-19, a three-bedroom apartment in San Francisco could cost $2,000,000. However, at the beginning of COVID-19, many people left San Francisco, so the same apartment would cost only $1,500,000. So even though the distribution of house features remains the same, the conditional distribution of the price of a house given its features has changed.

In many cases, **concept drifts are cyclic or seasonal**. For example, rideshare prices will fluctuate on weekdays versus weekends, and flight ticket prices rise during holiday seasons. Companies might have different models to deal with cyclic and seasonal drifts. For example, they might have one model to predict rideshare prices on weekdays and another model for weekends.


19. **General Data Distribution Shifts**

One of the general data distribution shifts is **feature change**, such as when new features are added, older features are
removed, or the set of all possible values of a feature changes.28 For example, your
model was using years for the “age” feature, but now it uses months, so the range
of this feature’s values has drifted. One time, our team realized that our model’s
performance plummeted because a bug in our pipeline caused a feature to become
NaNs (short for “not a number”).

**Label schema change** is when the set of possible values for Y change. With label shift,
P(Y) changes but P(X|Y) remains the same. With label schema change, both P(Y) and
P(X|Y) change. A schema describes the structure of the data, so the label schema of
a task describes the structure of the labels of that task. For example, a dictionary that
maps from a class to an integer value, such as {“POSITIVE”: 0, “NEGATIVE”: 1}, is a
schema.

With regression tasks, label schema change could happen because of changes in the
possible range of label values. Imagine you’re building a model to predict someone’s
credit score. Originally, you used a credit score system that ranged from 300 to 850,
but you switched to a new system that ranges from 250 to 900.

With classification tasks, **label schema change could happen because you have new
classes.** For example, suppose you are building a model to diagnose diseases and
there’s a new disease to diagnose. Classes can also become outdated or more finegrained.
Imagine that you’re in charge of a sentiment analysis model for tweets
that mention your brand. Originally, your model predicted only three classes: POSITIVE,
NEGATIVE, and NEUTRAL. However, your marketing department realized
the most damaging tweets are the angry ones, so they wanted to break the NEGATIVE
class into two classes: SAD and ANGRY. Instead of having three classes,
your task now has four classes. When the number of classes changes, your model’s
structure might change,29 and you might need to both relabel your data and
retrain your model from scratch. Label schema change is especially common with
high-cardinality tasks—tasks with a high number of classes—such as product or
documentation categorization.

There’s no rule that says that only one type of shift should happen at one time. A
model might suffer from multiple types of drift, which makes handling them a lot
more difficult.

 20. **Detecting Data Distribution Shifts**

Data distribution shifts are only a problem if they cause **your model’s performance
to degrade**. So the first idea might be to monitor your **model’s accuracy-related metrics—
accuracy, F1 score, recall, AUC-ROC, etc.—in production to see whether they
have changed.** “Change” here usually means **“decrease,”** but if my model’s accuracy
suddenly goes up or fluctuates significantly for no reason that I’m aware of, I’d want
to **investigate.**

Accuracy-related metrics work by comparing the model’s predictions to ground truth
labels.30 During model development, you have access to labels, **but in production,
you don’t always have access to labels, and even if you do, labels will be delayed,** as
discussed in the section “Natural Labels” on page 91. Having access to labels within a
reasonable time window will vastly help with giving you visibility into your model’s
performance.

When ground truth labels are unavailable or too delayed to be useful, we can monitor
other distributions of interest instead. The **distributions of interest are the input
distribution P(X), the label distribution P(Y), and the conditional distributions P(X|
Y) and P(Y|X).**

While we don’t need to know the ground truth labels Y to monitor the **input distribution,
monitoring the label distribution and both of the conditional distributions
require knowing Y.** 

In the industry, most drift detection methods focus on detecting **changes in the input distribution,** especially the
**distributions of features**, as we discuss in detail in this chapter.

**20.1. Statistical methods**

In industry, a simple method many companies use to detect whether the two distributions
are the same is to **compare their statistics like min, max, mean, median,
variance, various quantiles (such as 5th, 25th, 75th, or 95th quantile), skewness,
kurtosis, etc**. For example, you can compute the median and variance of the values
of a feature during inference and compare them to the metrics computed during
training. 

This is a good start, but these
metrics are far from sufficient.31 Mean, median, and variance are only useful with
the distributions for which the mean/median/variance are useful summaries. If those
metrics differ significantly, the inference distribution might have shifted from the
training distribution. **However, if those metrics are similar, there’s no guarantee that
there’s no shift.**

A **more sophisticated solution is to use a two-sample hypothesis test, shortened as
two-sample test.** It’s a test to determine whether the difference between two populations
(two sets of data) is statistically significant. **If the difference is statistically
significant, then the probability that the difference is a random fluctuation due to
sampling variability is very low, and, therefore, the difference is caused by the fact
that these two populations come from two distinct distributions.** If you consider the
data from yesterday to be the source population and the data from today to be the
target population and they are statistically different, it’s likely that the underlying data
distribution has shifted between yesterday and today.

A caveat is that just because the difference is statistically significant doesn’t mean that
it is practically important. However, a good heuristic is that if you are able to detect
the difference from a relatively small sample, then it is probably a serious difference.
If it takes a huge number of samples to detect, then the difference is probably not
worth worrying about.

**A basic two-sample test is the Kolmogorov–Smirnov test, also known as the K-S
or KS test.** It’s a **nonparametric statistical test, which means it doesn’t require any
parameters of the underlying distribution to work**. It doesn’t make any assumption
about the underlying distribution, which means it can work for any distribution.
However, one major drawback of the KS test is that it can only be used for **onedimensional
data.** If your model’s predictions and labels are one-dimensional (scalar
numbers), then the KS test is useful to detect label or prediction shifts. However, it
won’t work for high-dimensional data, and features are usually high-dimensional.33
KS tests can also be expensive and produce too many false positive alerts.34

Another test is **Least-Squares Density Difference,** an algorithm that is based on the
least squares density-difference estimation method.35 There is also MMD, Maximum
Mean Discrepancy (Gretton et al. 2012), a kernel-based technique for multivariate
two-sample testing and its variant Learned Kernel MMD (Liu et al. 2020). Alibi Detect is a great open source package with the
implementations of many drift detection algorithms, as shown in Figure 8-2.
**Because two-sample tests often work better on low-dimensional data than on highdimensional
data, it’s highly recommended that you reduce the dimensionality of
your data before performing a two-sample test on it.36**

![](https://github.com/DanialArab/images/blob/main/Designing_ML_Systems/Fig_8_2_drify_detectio_algos.png)

21. **Time scale windows for detecting shifts**

HERE














<a name="9"></a>
## CHAPTER 9: Continual Learning and Test in Production

<a name="10"></a>
## CHAPTER 10: Infrastructure and Tooling for MLOps

<a name="11"></a>
## CHAPTER 11: The Human Side of Machine Learning
